# social_sentiment_dashboard.py
"""
Real-Time Reddit Sentiment Dashboard  ğŸ“¢ğŸ“ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Enter one or more stock tickers.  
The app scrapes the 100 most-recent Reddit posts for each ticker
(no API key; uses Redditâ€™s public JSON endpoint), runs VADER sentiment,
and streams a live score chart that updates on refresh.

> Fast + free; perfect demo for NLP streaming work.  
> For production pipelines (auth, rate-limit back-off, FinBERT, Kafka),
  contact me â†’ https://drtomharty.com/bio
"""
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import requests, time, datetime as dt
import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px
from nltk.sentiment import SentimentIntensityAnalyzer
import feedparser, time
import nltk, os

# ensure VADER lexicon (downloaded once, cached)
nltk.download("vader_lexicon", quiet=True)
sia = SentimentIntensityAnalyzer()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ scraping helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}


def fetch_reddit_posts(query, limit=100):
    """
    Fallback via Reddit RSS feed:
    Parses https://www.reddit.com/search.rss?q=<query>&limit=<limit>
    """
    rss_url = f"https://www.reddit.com/search.rss?q={query}&limit={limit}"
    feed = feedparser.parse(rss_url)
    posts = []
    for entry in feed.entries:
        # published_parsed is a struct_time
        ts = time.mktime(entry.published_parsed)
        text = entry.title + " " + entry.get("summary","")
        posts.append((ts, text))
    return posts

def score_posts(posts):
    rows = []
    for ts, text in posts:
        score = sia.polarity_scores(text)["compound"]
        rows.append({"timestamp": dt.datetime.utcfromtimestamp(ts), "text": text, "compound": score})
    return pd.DataFrame(rows)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Reddit Sentiment Dashboard", layout="wide")
st.title("ğŸ“¢ Real-Time Reddit Sentiment Dashboard")

st.info(
    "ğŸ”” **Demo Notice**  \n"
    "Free, unauthenticated scrape of Redditâ€™s public JSON; suitable for demos. "
    "For enterprise-grade streaming NLP stacks, [contact me](https://drtomharty.com/bio).",
    icon="ğŸ’¡"
)

tickers = st.text_input("Enter comma-separated stock tickers (e.g., TSLA, NVDA, GME)",
                        value="TSLA").upper().replace(" ","").split(",")
limit = st.slider("Posts per ticker", 20, 100, 60, 10)

if st.button("ğŸš€ Fetch sentiment"):
    all_dfs = []
    with st.spinner("Scraping & scoringâ€¦"):
        for tkr in tickers:
            posts = fetch_reddit_posts(tkr, limit=limit)
            if not posts:
                continue
            df = score_posts(posts)
            df["ticker"] = tkr
            all_dfs.append(df)

    if not all_dfs:
        st.warning("No posts found. Try different ticker or higher limit.")
        st.stop()

    data = pd.concat(all_dfs)

    # â”€â”€â”€ Chart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("Average sentiment over time")
    # resample per hour for smoother line
    data.set_index("timestamp", inplace=True)
    hourly = data.groupby("ticker").resample("1H")["compound"].mean().reset_index()
    fig = px.line(hourly, x="timestamp", y="compound", color="ticker",
                  title="Hourly Avg VADER Compound Score (â†‘ positive, â†“ negative)",
                  markers=True)
    st.plotly_chart(fig, use_container_width=True)

    # â”€â”€â”€ Table & download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("Raw scored posts")
    st.dataframe(data.reset_index())

    st.download_button("â¬‡ï¸ Download scored data CSV",
                       data.reset_index().to_csv(index=False).encode(),
                       file_name="reddit_sentiment.csv",
                       mime="text/csv")
